# Smart Research Assistant

AI-powered document Q&A system built with **LangChain** and **RAG (Retrieval-Augmented Generation)**.

Upload PDFs, ask questions, get answers with sources.

## Tech Stack

**Backend:**
- FastAPI
- **LangChain** (document processing, RAG chains)
- **FAISS** (vector storage)
- **Groq API** (Llama models)

**Frontend:**
- Next.js
- Tailwind CSS
- shadcn/ui

## LangChain Components Used

- `PyPDFLoader` - Extract text from PDFs
- `RecursiveCharacterTextSplitter` - Split documents into chunks
- `FAISS` - Store and search document embeddings
- `RetrievalQA` - RAG chain for Q&A
- Custom Groq chat model integration

## Setup

### Backend
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Linux/Mac
pip install -r requirements.txt

# Add your Groq API key to .env
echo "GROK_API_KEY=your_key_here" > .env

uvicorn app.main:app --reload
```

### Frontend
```bash
cd frontend
npm install
npm run dev
```

Visit http://localhost:3000

## How It Works

1. **Upload PDF** → LangChain processes and chunks the document
2. **Ask Question** → FAISS finds relevant chunks
3. **Get Answer** → Groq LLM generates response with sources

## API Endpoints

- `POST /documents/upload` - Upload PDF
- `GET /documents/` - List documents  
- `POST /documents/{id}/summarize` - Summarize document
- `POST /chat/` - Ask questions

## Requirements

- Python 3.8+
- Node.js 18+
- Groq API key (free at console.groq.com)